{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import brightway2 as bc\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "# Mock Data\n",
    "unique_process_index = ['proc1', 'proc2', 'proc3']\n",
    "impact_categories = [('Impact1', 'Test Impact 1'), ('Impact2', 'Test Impact 2')]\n",
    "uniquie_process_dct = {'proc1': 1, 'proc2': 1, 'proc3': 1}\n",
    "\n",
    "# Expected DataFrame\n",
    "df_expected = pd.DataFrame({\n",
    "    'Impact1': [10, 20, 30],\n",
    "    'Impact2': [15, 25, 35]\n",
    "}, index=unique_process_index)\n",
    "\n",
    "# Set the start method to 'spawn' (default for Windows)\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_lca_calculation(args):\n",
    "    key, item, impact = args\n",
    "    print(f\"Running LCA for {key} {item}\")\n",
    "    # Mock return value based on the impact category\n",
    "    if impact[0] == 'Impact1':\n",
    "        return key, impact, {'proc1': 10, 'proc2': 20, 'proc3': 30}.get(key, None)\n",
    "    elif impact[0] == 'Impact2':\n",
    "        return key, impact, {'proc1': 15, 'proc2': 25, 'proc3': 35}.get(key, None)\n",
    "    return key, impact, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LCA for proc1 1\n",
      "Running LCA for proc2 1\n",
      "Running LCA for proc3 1\n",
      "Running LCA for proc1 1\n",
      "Running LCA for proc2 1\n",
      "Running LCA for proc3 1\n",
      "('proc1', ('Impact1', 'Test Impact 1'), 10)\n",
      "('proc2', ('Impact1', 'Test Impact 1'), 20)\n",
      "('proc3', ('Impact1', 'Test Impact 1'), 30)\n",
      "('proc1', ('Impact2', 'Test Impact 2'), 15)\n",
      "('proc2', ('Impact2', 'Test Impact 2'), 25)\n",
      "('proc3', ('Impact2', 'Test Impact 2'), 35)\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "tasks = [(str(key), int(item), tuple(impact)) for impact in impact_categories for key, item in uniquie_process_dct.items()]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(mock_lca_calculation, tasks))\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LCA for proc1 1\n",
      "Running LCA for proc2 1\n",
      "Running LCA for proc3 1\n",
      "Running LCA for proc1 1\n",
      "Running LCA for proc2 1\n",
      "Running LCA for proc3 1\n",
      "Basic Functionality Test Failed ❌\n",
      "Attributes of DataFrame.iloc[:, 0] (column name=\"Impact1\") are different\n",
      "\n",
      "Attribute \"dtype\" are different\n",
      "[left]:  object\n",
      "[right]: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare tasks for multiprocessing\n",
    "tasks = [(key, item, impact) for impact in impact_categories for key, item in uniquie_process_dct.items()]\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(mock_lca_calculation, tasks)\n",
    "\n",
    "# Create DataFrame to store results\n",
    "df_test = pd.DataFrame(0, index=unique_process_index, columns=[imp[0] for imp in impact_categories], dtype=object)\n",
    "\n",
    "# Assign results to DataFrame\n",
    "for key, impact, score in results:\n",
    "    df_test.at[key, impact[0]] = score\n",
    "    # print(key)\n",
    "\n",
    "# Check if the results match the expected DataFrame\n",
    "try:\n",
    "    pd.testing.assert_frame_equal(df_test, df_expected)\n",
    "    print(\"Basic Functionality Test Passed ✅\")\n",
    "except AssertionError as e:\n",
    "    print(\"Basic Functionality Test Failed ❌\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock LCA function with an intentional error for 'proc2'\n",
    "def mock_lca_error(args):\n",
    "    key, item, impact = args\n",
    "    if key == 'proc2':\n",
    "        raise ValueError(\"Mock LCA error\")\n",
    "    return mock_lca_calculation(args)\n",
    "\n",
    "# Run the tasks with error handling\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    results = pool.map(mock_lca_error, tasks)\n",
    "\n",
    "# Verify that 'proc2' returned None due to the error\n",
    "error_test_passed = all(\n",
    "    (key != 'proc2' and score is not None) or (key == 'proc2' and score is None)\n",
    "    for key, impact, score in results\n",
    ")\n",
    "\n",
    "if error_test_passed:\n",
    "    print(\"Error Handling Test Passed ✅\")\n",
    "else:\n",
    "    print(\"Error Handling Test Failed ❌\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serial Execution\n",
    "start_serial = time.time()\n",
    "serial_results = [mock_lca_calculation(task) for task in tasks]\n",
    "end_serial = time.time()\n",
    "serial_time = end_serial - start_serial\n",
    "print(f\"Serial execution time: {serial_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Execution\n",
    "start_parallel = time.time()\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    parallel_results = pool.map(mock_lca_calculation, tasks)\n",
    "end_parallel = time.time()\n",
    "parallel_time = end_parallel - start_parallel\n",
    "print(f\"Parallel execution time: {parallel_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parallel_time < serial_time:\n",
    "    print(f\"Parallel execution is faster by {serial_time - parallel_time:.4f} seconds ✅\")\n",
    "else:\n",
    "    print(f\"Serial execution is faster ❌ (unexpected)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using %timeit for performance measurement\n",
    "print(\"Serial execution timing with %timeit:\")\n",
    "%timeit [mock_lca_calculation(task) for task in tasks]\n",
    "\n",
    "print(\"\\nParallel execution timing with %timeit:\")\n",
    "%timeit with mp.Pool(processes=mp.cpu_count()) as pool: pool.map(mock_lca_calculation, tasks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
